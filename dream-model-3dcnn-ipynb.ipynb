{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5908559,"sourceType":"datasetVersion","datasetId":3392987},{"sourceId":5915874,"sourceType":"datasetVersion","datasetId":3397661},{"sourceId":5916189,"sourceType":"datasetVersion","datasetId":3397778},{"sourceId":5917831,"sourceType":"datasetVersion","datasetId":3398595}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nThis module implement util classes for reading and visalizing the DREAM dataset using Python 3. \n\"\"\"\nimport os\nimport json\nimport io\nimport numpy as np\nimport pandas as pd\nfrom math import sin,cos\nfrom functools import reduce\nimport cv2\nfrom matplotlib import pyplot as plt\n\ndef open(path):\n    with io.open(path) as f:\n        return Intervention(json.load(f))\n\ndef withlast(iterable):\n\ti = iter(iterable)\n\ttry:\n\t\tv = next(i)\n\t\tfor vv in i: \n\t\t\tyield v+(False,) if isinstance(v,tuple) else (v,False)\n\t\t\tv = vv\n\t\tyield v+(True,) if isinstance(v,tuple) else (v,True)\n\texcept StopIteration:\n\t\tpass\n\nclass Intervention(dict):\n\n    def __init__(self, data):\n        for key in data:\n            self[key] = self.__fixNan(data[key])\n\n    def __fixNan(self,data):\n        if data is None:\n            return float('nan')\n        elif isinstance(data,dict):\n            for key in data:\n                data[key] = self.__fixNan(data[key])\n        elif isinstance(data,list):\n            for i, v in enumerate(data):\n                data[i] = self.__fixNan(v)\n        return data\n\n    def __repr__(self):\n        return 'Intervention recording with {} samples'.format(self.sampleCount())\n\n    def sampleCount(self):\n        return len(self['skeleton']['head']['x'])\n\n    def structure(self,dic=None,linewidth=50,tab=1):\n        if dic is None: dic = self\n        s = ['{']\n        for key,val,islast in withlast(dic.items()):\n            if isinstance(val,dict):\n                s.append('\"{0}\": {1}{2}'.format(key, self.structure(val,linewidth,tab+1), '' if islast else ','))\n            elif isinstance(val,list):\n                s.append('\"{0}\": {1}{2}'.format(key, '[]', '' if islast else ','))\n            elif isinstance(val,str):\n            \ts.append('\"{0}\": \"{1}\"{2}'.format(key, val, '' if islast else ','))\n            elif val is None:\n                s.append('\"{0}\": null{2}'.format(key, '' if islast else ','))\n            else:\n                s.append('\"{0}\": {1}{2}'.format(key, val, '' if islast else ','))\n        if len(s)==1:\n            return \"{}\"\n        elif reduce(lambda n,s: n+len(s),s,0) < linewidth:\n            return ''.join(s) + '}'\n        else:\n            return ('\\n'+(' '*4*tab)).join(s) + '\\n' + (' '*4*(tab-1)) + '}'\n\n    def gaze(self,distance=1):\n        headPose = [np.array([v]).transpose() for v in zip(self['skeleton']['head']['x'],self['skeleton']['head']['y'],self['skeleton']['head']['z'])]\n        xrot = [self.xrot(gaze) for gaze in zip(self['head_gaze']['rx'],self['head_gaze']['ry'],self['head_gaze']['rz'])]\n        return [np.matmul(rot,pose) for rot,pose in zip(xrot,headPose)]\n        \n    def xrot(self,gaze):\n        v = gaze[0]\n        return np.array([[0,0,1],[-sin(v),cos(v),0],[cos(v),sin(v),0]])\n    \n    def to_csv(self,*args,**kwargs):\n        return self.to_dataFrame().to_csv(*args,**kwargs)\n    \n    def to_dataFrame(self):\n        return pd.DataFrame.from_dict(dict(self.columns()))\n    \n    def columns(self,d=None,parent_name='',trim=True):\n        if d is None: d=self\n        if trim == True: d.trim()\n        for k, v in d.items():\n            if '$' in k: continue\n            if isinstance(v,dict):\n                for c, cc in self.columns(v,parent_name + '_' + k if parent_name else k,False):\n                    yield c, cc\n            else:\n                yield parent_name + '_' + k if parent_name else k, v\n                \n    def trim(self):\n        \"\"\"Guarantees that all time dependent data has the same length, extending arrays where necessary.\"\"\"\n        length = 0\n        cols = list(self.columns(trim=False))\n        for c, v in cols:\n            if isinstance(v,list): length = max(length,len(v))\n        for c, v in cols:\n            while isinstance(v,list) and len(v) > 1 and len(v) < length:\n                v.append(float('nan'))","metadata":{"id":"USkgNjPwKJ39","execution":{"iopub.status.busy":"2024-04-18T15:08:39.430578Z","iopub.execute_input":"2024-04-18T15:08:39.431281Z","iopub.status.idle":"2024-04-18T15:08:39.457687Z","shell.execute_reply.started":"2024-04-18T15:08:39.431245Z","shell.execute_reply":"2024-04-18T15:08:39.456892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import dataset_tools as dataset\n\n# import pandas as pd\n# import numpy as np\n# os.listdir(home)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.459284Z","iopub.execute_input":"2024-04-18T15:08:39.459567Z","iopub.status.idle":"2024-04-18T15:08:39.472213Z","shell.execute_reply.started":"2024-04-18T15:08:39.459543Z","shell.execute_reply":"2024-04-18T15:08:39.471363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(df):\n    drop_list=[\"ados_preTest_communication\", \"ados_preTest_interaction\", \"ados_preTest_module\", \"ados_preTest_play\", \"ados_preTest_protocol\", \"ados_preTest_socialCommunicationQuestionnaire\", \"ados_preTest_stereotype\", \"condition\", \"frame_rate\", \"participant_gender\", \"participant_id\", \"skeleton_elbow_left_confidence\", \"skeleton_elbow_right_confidence\", \"skeleton_hand_left_confidence\", \"skeleton_hand_right_confidence\", \"skeleton_head_confidence\", \"skeleton_sholder_center_confidence\", \"skeleton_sholder_left_confidence\", \"skeleton_sholder_right_confidence\", \"skeleton_wrist_left_confidence\", \"skeleton_wrist_right_confidence\", \"task_ability\", \"task_difficultyLevel\", \"task_end\", \"task_index\", \"task_start\", \"time\"]\n    df.drop(drop_list,axis=1,inplace=True)\n    df.dropna(axis=0,inplace=True)\n    df=df[::5].reset_index(drop=True)\n    return df","metadata":{"id":"ekj5RUwXa1ra","execution":{"iopub.status.busy":"2024-04-18T15:08:39.473327Z","iopub.execute_input":"2024-04-18T15:08:39.473666Z","iopub.status.idle":"2024-04-18T15:08:39.483944Z","shell.execute_reply.started":"2024-04-18T15:08:39.473635Z","shell.execute_reply":"2024-04-18T15:08:39.483121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# data=open(\"/kaggle/input/dream-dataset-part1/Part 1 Users/User 11/User 11_2_diagnosis abilities_20170315_090533.665000.json\")\n# df=preprocess(data.to_dataFrame())\n# # print(df.head())\n# # fig, ax = plt.subplots(1,1,figsize=(12, 7))\n# skeleton = data['skeleton']\n# frame = 25*16\n# for key in skeleton:\n#     print(skeleton[key]['x'][frame],skeleton[key]['y'][frame])\n#     x,y = skeleton[key]['x'][frame],skeleton[key]['y'][frame]\n#     ax.plot(x,y,'o')\n#     ax.text(x,y,key)\n# ax.axis([-300,500,-700,0])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.485861Z","iopub.execute_input":"2024-04-18T15:08:39.486139Z","iopub.status.idle":"2024-04-18T15:08:39.498543Z","shell.execute_reply.started":"2024-04-18T15:08:39.486116Z","shell.execute_reply":"2024-04-18T15:08:39.497850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # import matplotlib.pyplot as plt\n\n# # Coordinates\n# coordinates = [\n#     (-146.001, -564.381281),\n#     (56.9436, -579.478134),\n#     (-6.61139, -674.218252),\n#     (262.33, -650.46385),\n#     (-19.8384, -272.785714),\n#     (-38.4072, -420.667455)\n# ]\n\n# # Image size\n# image_size = 256\n\n# # Calculate the range of the coordinates\n# x_min = min(coord[0] for coord in coordinates)\n# x_max = max(coord[0] for coord in coordinates)\n# y_min = min(coord[1] for coord in coordinates)\n# y_max = max(coord[1] for coord in coordinates)\n\n# # Calculate the scale factors for the transformation\n# x_scale = image_size / (x_max - x_min)\n# y_scale = image_size / (y_max - y_min)\n\n# # Create a new figure and axis\n# fig, ax = plt.subplots()\n\n# # Set the limits of the plot\n# ax.set_xlim(0, image_size)\n# ax.set_ylim(0, image_size)\n\n# # Plot each coordinate as a dot after applying the transformation\n# for coord in coordinates:\n#     x, y = coord\n#     # Apply the transformation\n#     x_pixel = (x - x_min) * x_scale\n#     y_pixel = (y - y_min) * y_scale\n#     ax.plot(x_pixel, y_pixel, 'bo')  # 'bo' represents blue dots\n\n# # Display the plot\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.499580Z","iopub.execute_input":"2024-04-18T15:08:39.499831Z","iopub.status.idle":"2024-04-18T15:08:39.508626Z","shell.execute_reply.started":"2024-04-18T15:08:39.499808Z","shell.execute_reply":"2024-04-18T15:08:39.507719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import cv2\n# import numpy as np\n\n# # Define the image size and resolution\n# image_width = 256\n# image_height = 256\n# image_channels = 3  # Assuming an RGB image, change if necessary\n\n# # Create an empty image canvas\n# image = np.zeros((image_height, image_width, image_channels), dtype=np.uint8)\n\n# # Define skeleton colors\n# skeleton_color = (0, 255, 0)  # Green color\n\n# # Define gaze color\n# gaze_color = (0, 0, 255)  # Red color\n\n# # Example skeleton data (assuming 2D coordinates)\n# skeleton_data = {\n#     'head': (50, 50),\n#     'shoulder': (50, 100),\n#     'hand_left': (30, 120),\n#     'hand_right': (70, 120),\n#     'leg_left': (40, 200),\n#     'leg_right': (60, 200)\n# }\n\n# # Example gaze data (assuming gaze direction coordinates)\n# gaze_data = {\n#     'eye_left': (45, 55),\n#     'eye_right': (55, 55),\n#     'gaze_direction': (50, 40)\n# }\n\n# # Draw skeleton on the image canvas\n# for joint, (x, y) in skeleton_data.items():\n#     cv2.circle(image, (x, y), 3, skeleton_color, -1)\n#     cv2.putText(image, joint, (x + 5, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, skeleton_color, 2)\n\n# # Draw gaze markers on the image canvas\n# for eye in ['eye_left', 'eye_right']:\n#     eye_x, eye_y = gaze_data[eye]\n#     cv2.circle(image, (eye_x, eye_y), 2, gaze_color, -1)\n\n# gaze_direction_x, gaze_direction_y = gaze_data['gaze_direction']\n# cv2.arrowedLine(image, (gaze_direction_x, gaze_direction_y), (eye_x, eye_y), gaze_color, 2)\n\n# # Display the image\n# cv2.imshow('Image Representation', image)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.509662Z","iopub.execute_input":"2024-04-18T15:08:39.509986Z","iopub.status.idle":"2024-04-18T15:08:39.521820Z","shell.execute_reply.started":"2024-04-18T15:08:39.509955Z","shell.execute_reply":"2024-04-18T15:08:39.520999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image = np.zeros((image_height, image_width), dtype=np.uint8)\n# cv2.circle(image, (50, 50), 1, 255, -1)\n# plt.imshow(image)\n# cv2.imwrite(\"output.png\",image)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.522886Z","iopub.execute_input":"2024-04-18T15:08:39.523211Z","iopub.status.idle":"2024-04-18T15:08:39.535315Z","shell.execute_reply.started":"2024-04-18T15:08:39.523181Z","shell.execute_reply":"2024-04-18T15:08:39.534490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_size=36\n# def bound(df):\n#     for c in list(df.columns)[1:]:\n#         mi=np.min(df[c])\n#         ma=np.max(df[c])\n#         scale = image_size / (ma - mi)\n#         df[c]=np.int32((df[c]-mi)*scale)\n#     return df\n\n# def getImage(df):\n#     ans=[]\n#     for index, d in df.iterrows():\n#         image = np.zeros((image_height, image_width), dtype=np.uint8)\n#         cv2.circle(image, (d['skeleton_elbow_left_x'], d['skeleton_elbow_left_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_elbow_right_x'], d['skeleton_elbow_right_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_hand_left_x'], d['skeleton_hand_left_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_hand_right_x'], d['skeleton_hand_right_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_head_x'], d['skeleton_head_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_sholder_center_x'], d['skeleton_sholder_center_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_sholder_left_x'], d['skeleton_sholder_left_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_sholder_right_x'], d['skeleton_sholder_right_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_wrist_left_x'], d['skeleton_wrist_left_y']), 1, 255, -1)\n#         cv2.circle(image, (d['skeleton_wrist_right_x'], d['skeleton_wrist_right_y']), 1, 255, -1)\n        \n#         cv2.line(image,(d['skeleton_head_x'], d['skeleton_head_y']),(d['skeleton_sholder_center_x'], d['skeleton_sholder_center_y']),255)\n#         cv2.line(image,(d['skeleton_sholder_left_x'], d['skeleton_sholder_left_y']),(d['skeleton_sholder_center_x'], d['skeleton_sholder_center_y']),255)\n#         cv2.line(image,(d['skeleton_sholder_right_x'], d['skeleton_sholder_right_y']),(d['skeleton_sholder_center_x'], d['skeleton_sholder_center_y']),255)\n#         cv2.line(image,(d['skeleton_sholder_left_x'], d['skeleton_sholder_left_y']),(d['skeleton_elbow_left_x'], d['skeleton_elbow_left_y']),255)\n#         cv2.line(image,(d['skeleton_hand_left_x'], d['skeleton_hand_left_y']),(d['skeleton_elbow_left_x'], d['skeleton_elbow_left_y']),255)\n#         cv2.line(image,(d['skeleton_hand_left_x'], d['skeleton_hand_left_y']),(d['skeleton_wrist_left_x'], d['skeleton_wrist_left_y']),255)\n        \n#         cv2.line(image,(d['skeleton_sholder_right_x'], d['skeleton_sholder_right_y']),(d['skeleton_elbow_right_x'], d['skeleton_elbow_right_y']),255)\n#         cv2.line(image,(d['skeleton_hand_right_x'], d['skeleton_hand_right_y']),(d['skeleton_elbow_right_x'], d['skeleton_elbow_right_y']),255)\n#         cv2.line(image,(d['skeleton_hand_right_x'], d['skeleton_hand_right_y']),(d['skeleton_wrist_right_x'], d['skeleton_wrist_right_y']),255)\n        \n#         ans.append(image)\n#     return ans\n\n\n# df=bound(df.head())\n# # video=getImage(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.589113Z","iopub.execute_input":"2024-04-18T15:08:39.589393Z","iopub.status.idle":"2024-04-18T15:08:39.594801Z","shell.execute_reply.started":"2024-04-18T15:08:39.589369Z","shell.execute_reply":"2024-04-18T15:08:39.593909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size=30\n# def bound(df):\n#     for c in list(df.columns)[1:]:\n#         mi=np.min(df[c])\n#         ma=np.max(df[c])\n#         scale = image_size / (ma - mi)\n#         df[c]=(df[c]-mi)*scale\n#     return df\n\ndef getImage(df):\n    ans=[]\n    for index, d in df.iterrows():\n        a=['skeleton_elbow_left_x','skeleton_elbow_left_y','skeleton_elbow_right_x','skeleton_elbow_right_y','skeleton_hand_left_x','skeleton_hand_left_y','skeleton_hand_right_x','skeleton_hand_right_y','skeleton_head_x','skeleton_head_y','skeleton_sholder_center_x','skeleton_sholder_center_y','skeleton_sholder_left_x','skeleton_sholder_left_y','skeleton_sholder_right_x','skeleton_sholder_right_y','skeleton_wrist_left_x','skeleton_wrist_left_y','skeleton_wrist_right_x','skeleton_wrist_right_y']\n        mi=np.min(d[a])\n        ma=np.max(d[a])\n        if(mi==0 and ma==0):\n            ans.append(np.zeros((image_size, image_size, 1), dtype=np.uint8))\n            continue\n        scale = image_size / (ma - mi)\n        d[a]=(d[a]-mi)*scale\n        image = np.zeros((image_size, image_size, 1), dtype=np.uint8)\n        cv2.circle(image, (int(d['skeleton_elbow_left_x']), int(d['skeleton_elbow_left_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_elbow_right_x']), int(d['skeleton_elbow_right_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_hand_left_x']), int(d['skeleton_hand_left_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_hand_right_x']), int(d['skeleton_hand_right_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_head_x']), int(d['skeleton_head_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_sholder_center_x']), int(d['skeleton_sholder_center_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_sholder_left_x']), int(d['skeleton_sholder_left_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_sholder_right_x']), int(d['skeleton_sholder_right_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_wrist_left_x']), int(d['skeleton_wrist_left_y'])), 1, 255, -1)\n        cv2.circle(image, (int(d['skeleton_wrist_right_x']), int(d['skeleton_wrist_right_y'])), 1, 255, -1)\n        \n        cv2.line(image,(int(d['skeleton_head_x']), int(d['skeleton_head_y'])),(int(d['skeleton_sholder_center_x']), int(d['skeleton_sholder_center_y'])),255)\n        cv2.line(image,(int(d['skeleton_sholder_left_x']), int(d['skeleton_sholder_left_y'])),(int(d['skeleton_sholder_center_x']), int(d['skeleton_sholder_center_y'])),255)\n        cv2.line(image,(int(d['skeleton_sholder_right_x']), int(d['skeleton_sholder_right_y'])),(int(d['skeleton_sholder_center_x']), int(d['skeleton_sholder_center_y'])),255)\n        cv2.line(image,(int(d['skeleton_sholder_left_x']), int(d['skeleton_sholder_left_y'])),(int(d['skeleton_elbow_left_x']), int(d['skeleton_elbow_left_y'])),255)\n        cv2.line(image,(int(d['skeleton_hand_left_x']), int(d['skeleton_hand_left_y'])),(int(d['skeleton_elbow_left_x']), int(d['skeleton_elbow_left_y'])),255)\n        cv2.line(image,(int(d['skeleton_hand_left_x']), int(d['skeleton_hand_left_y'])),(int(d['skeleton_wrist_left_x']), int(d['skeleton_wrist_left_y'])),255)\n        \n        cv2.line(image,(int(d['skeleton_sholder_right_x']), int(d['skeleton_sholder_right_y'])),(int(d['skeleton_elbow_right_x']), int(d['skeleton_elbow_right_y'])),255)\n        cv2.line(image,(int(d['skeleton_hand_right_x']), int(d['skeleton_hand_right_y'])),(int(d['skeleton_elbow_right_x']), int(d['skeleton_elbow_right_y'])),255)\n        cv2.line(image,(int(d['skeleton_hand_right_x']), int(d['skeleton_hand_right_y'])),(int(d['skeleton_wrist_right_x']), int(d['skeleton_wrist_right_y'])),255)\n        \n        ans.append(image)\n    return ans\n\n\n# df=bound(df.head())\n# video=getImage(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.596876Z","iopub.execute_input":"2024-04-18T15:08:39.597362Z","iopub.status.idle":"2024-04-18T15:08:39.621045Z","shell.execute_reply.started":"2024-04-18T15:08:39.597331Z","shell.execute_reply":"2024-04-18T15:08:39.620212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list(df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.622000Z","iopub.execute_input":"2024-04-18T15:08:39.622293Z","iopub.status.idle":"2024-04-18T15:08:39.633397Z","shell.execute_reply.started":"2024-04-18T15:08:39.622269Z","shell.execute_reply":"2024-04-18T15:08:39.632373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ['skeleton_elbow_left_x',\n#  'skeleton_elbow_left_y',\n#  'skeleton_elbow_left_z',\n#  'skeleton_elbow_right_x',\n#  'skeleton_elbow_right_y',\n#  'skeleton_elbow_right_z',\n#  'skeleton_hand_left_x',\n#  'skeleton_hand_left_y',\n#  'skeleton_hand_left_z',\n#  'skeleton_hand_right_x',\n#  'skeleton_hand_right_y',\n#  'skeleton_hand_right_z',\n#  'skeleton_head_x',\n#  'skeleton_head_y',\n#  'skeleton_head_z',\n#  'skeleton_sholder_center_x',\n#  'skeleton_sholder_center_y',\n#  'skeleton_sholder_center_z',\n#  'skeleton_sholder_left_x',\n#  'skeleton_sholder_left_y',\n#  'skeleton_sholder_left_z',\n#  'skeleton_sholder_right_x',\n#  'skeleton_sholder_right_y',\n#  'skeleton_sholder_right_z',\n#  'skeleton_wrist_left_x',\n#  'skeleton_wrist_left_y',\n#  'skeleton_wrist_left_z',\n#  'skeleton_wrist_right_x',\n#  'skeleton_wrist_right_y',\n#  'skeleton_wrist_right_z']","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.634405Z","iopub.execute_input":"2024-04-18T15:08:39.634745Z","iopub.status.idle":"2024-04-18T15:08:39.641838Z","shell.execute_reply.started":"2024-04-18T15:08:39.634714Z","shell.execute_reply":"2024-04-18T15:08:39.640988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for index, d in df.iterrows():\n#         image = np.zeros((image_height, image_width), dtype=np.uint8)\n#         print(type(d['skeleton_elbow_left_x']))\n#         print(int(d['skeleton_elbow_left_x']))\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.644685Z","iopub.execute_input":"2024-04-18T15:08:39.645012Z","iopub.status.idle":"2024-04-18T15:08:39.655340Z","shell.execute_reply.started":"2024-04-18T15:08:39.644980Z","shell.execute_reply":"2024-04-18T15:08:39.654638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# video=getImage(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.656691Z","iopub.execute_input":"2024-04-18T15:08:39.657028Z","iopub.status.idle":"2024-04-18T15:08:39.664820Z","shell.execute_reply.started":"2024-04-18T15:08:39.656999Z","shell.execute_reply":"2024-04-18T15:08:39.664010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a=abs((df['skeleton_wrist_right_y'].head())*0)\n# type(a)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.666232Z","iopub.execute_input":"2024-04-18T15:08:39.666559Z","iopub.status.idle":"2024-04-18T15:08:39.674608Z","shell.execute_reply.started":"2024-04-18T15:08:39.666529Z","shell.execute_reply":"2024-04-18T15:08:39.673728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(video[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.675659Z","iopub.execute_input":"2024-04-18T15:08:39.675925Z","iopub.status.idle":"2024-04-18T15:08:39.683071Z","shell.execute_reply.started":"2024-04-18T15:08:39.675902Z","shell.execute_reply":"2024-04-18T15:08:39.682243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dirs = os.listdir(\"/kaggle/input/dream-dataset-part1/Part 1 Users\")\nprint(dirs)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:08:39.683977Z","iopub.execute_input":"2024-04-18T15:08:39.684269Z","iopub.status.idle":"2024-04-18T15:08:39.695292Z","shell.execute_reply.started":"2024-04-18T15:08:39.684246Z","shell.execute_reply":"2024-04-18T15:08:39.694424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos=[]\nhome=\"/kaggle/input/dream-dataset-part1/Part 1 Users\"\n\nper_user=10\ny=[]\nfor dir in os.listdir(home):\n    n=0\n    for dirname, _, filenames in os.walk(os.path.join(home, dir)):\n        for filename in filenames:\n            if(n==per_user):\n                break\n            data=open(os.path.join(dirname, filename))\n#             print(os.path.join(dirname, filename))\n            df=data.to_dataFrame()\n            df=preprocess(df)\n            if(df.shape[0]==0):\n                continue\n#             print(df.shape)\n            y.append(df[\"ados_preTest_total\"][0])\n            videos.append(df.drop([\"ados_preTest_total\"], axis=1))\n            n+=1\n","metadata":{"id":"nOuJ40xgTvgV","outputId":"d5e60600-053a-46fb-8ab6-84ba62b615a3","execution":{"iopub.status.busy":"2024-04-18T15:08:39.696397Z","iopub.execute_input":"2024-04-18T15:08:39.696681Z","iopub.status.idle":"2024-04-18T15:09:43.550179Z","shell.execute_reply.started":"2024-04-18T15:08:39.696658Z","shell.execute_reply":"2024-04-18T15:09:43.549279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# home=\"/kaggle/input/dream-dataset-part-2/Part 2 Users\"\n# per_user=3\n# for dir in os.listdir(home):\n# #     n=0\n#     for dirname, _, filenames in os.walk(os.path.join(home, dir)):\n#         for filename in filenames:\n# #             if(n==per_user):\n# #                 break\n#             data=open(os.path.join(dirname, filename))\n# #             print(os.path.join(dirname, filename))\n#             df=data.to_dataFrame()\n#             df=preprocess(df)\n#             if(df.shape[0]==0):\n#                 continue\n# #             print(df.shape)\n#             videos.append(df)\n# #             n+=1\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:43.551382Z","iopub.execute_input":"2024-04-18T15:09:43.551729Z","iopub.status.idle":"2024-04-18T15:09:43.556730Z","shell.execute_reply.started":"2024-04-18T15:09:43.551700Z","shell.execute_reply":"2024-04-18T15:09:43.555508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# home=\"/kaggle/input/dream-dataset-part-3/Part 3 Users\"\n# per_user=3\n# for dir in os.listdir(home):\n# #     n=0\n#     for dirname, _, filenames in os.walk(os.path.join(home, dir)):\n#         for filename in filenames:\n# #             if(n==per_user):\n# #                 break\n#             data=open(os.path.join(dirname, filename))\n# #             print(os.path.join(dirname, filename))\n#             df=data.to_dataFrame()\n#             df=preprocess(df)\n#             if(df.shape[0]==0):\n#                 continue\n# #             print(df.shape)\n#             videos.append(df)\n# #             n+=1\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:43.557811Z","iopub.execute_input":"2024-04-18T15:09:43.558061Z","iopub.status.idle":"2024-04-18T15:09:43.573411Z","shell.execute_reply.started":"2024-04-18T15:09:43.558038Z","shell.execute_reply":"2024-04-18T15:09:43.572589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# home=\"/kaggle/input/dream-dataset-part-4/Part 4 Users\"\n# per_user=3\n# for dir in os.listdir(home):\n# #     n=0\n#     for dirname, _, filenames in os.walk(os.path.join(home, dir)):\n#         for filename in filenames:\n# #             if(n==per_user):\n# #                 break\n#             data=open(os.path.join(dirname, filename))\n# #             print(os.path.join(dirname, filename))\n#             df=data.to_dataFrame()\n#             df=preprocess(df)\n#             if(df.shape[0]==0):\n#                 continue\n# #             print(df.shape)\n#             videos.append(df)\n# #             n+=1\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:43.574476Z","iopub.execute_input":"2024-04-18T15:09:43.574772Z","iopub.status.idle":"2024-04-18T15:09:43.583916Z","shell.execute_reply.started":"2024-04-18T15:09:43.574738Z","shell.execute_reply":"2024-04-18T15:09:43.583109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(videos))\nmax_frames=100\npadded_dataframes = []\nfor i in range(len(videos)):\n    max_frames = max(max_frames,len(videos[i]))\nprint(max_frames)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:43.587792Z","iopub.execute_input":"2024-04-18T15:09:43.588071Z","iopub.status.idle":"2024-04-18T15:09:43.597208Z","shell.execute_reply.started":"2024-04-18T15:09:43.588047Z","shell.execute_reply":"2024-04-18T15:09:43.596190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos[1].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:43.598584Z","iopub.execute_input":"2024-04-18T15:09:43.598880Z","iopub.status.idle":"2024-04-18T15:09:43.610772Z","shell.execute_reply.started":"2024-04-18T15:09:43.598855Z","shell.execute_reply":"2024-04-18T15:09:43.609902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getBest(df):\n    diff=np.linalg.norm(df.diff(axis=0).drop(0), axis=1)\n    ind=diff.argmax()\n    return ind","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:43.611827Z","iopub.execute_input":"2024-04-18T15:09:43.612135Z","iopub.status.idle":"2024-04-18T15:09:43.620673Z","shell.execute_reply.started":"2024-04-18T15:09:43.612105Z","shell.execute_reply":"2024-04-18T15:09:43.619905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_frames=4000\n# padded_dataframes = []\n\nfor i in range(len(videos)):\n    padding_length = max_frames - len(videos[i])\n    if(padding_length<=0):\n        videos[i] = (videos[i][:max_frames]).to_numpy()\n#         ind = getBest(videos[i])\n#         if(ind-max_frames//2>-1 and ind+max_frames//2<len(videos[i])):\n#             videos[i]=(videos[i][ind-max_frames//2:ind+max_frames//2]).reset_index(drop=True)\n#         elif(ind-max_frames//2<0):\n#             videos[i]=(videos[i][:max_frames]).reset_index(drop=True)\n#         else:\n#             videos[i]=(videos[i][-max_frames:]).reset_index(drop=True)\n    else:\n        videos[i] = (pd.concat([videos[i], pd.DataFrame(np.zeros((padding_length, videos[i].shape[1])), columns=videos[i].columns)]).reset_index(drop=True)).to_numpy()\n    \n# videos[0][\"ados_preTest_total\"]","metadata":{"id":"x5alqkc4vzFt","execution":{"iopub.status.busy":"2024-04-18T15:09:43.621692Z","iopub.execute_input":"2024-04-18T15:09:43.622021Z","iopub.status.idle":"2024-04-18T15:09:43.956551Z","shell.execute_reply.started":"2024-04-18T15:09:43.621988Z","shell.execute_reply":"2024-04-18T15:09:43.955453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos=np.array(videos)\ny=np.array(y)\nprint(videos.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:43.957780Z","iopub.execute_input":"2024-04-18T15:09:43.958136Z","iopub.status.idle":"2024-04-18T15:09:44.063731Z","shell.execute_reply.started":"2024-04-18T15:09:43.958103Z","shell.execute_reply":"2024-04-18T15:09:44.062837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"FGZB4aNRVxRh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[3]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:44.064692Z","iopub.execute_input":"2024-04-18T15:09:44.064958Z","iopub.status.idle":"2024-04-18T15:09:44.070636Z","shell.execute_reply.started":"2024-04-18T15:09:44.064935Z","shell.execute_reply":"2024-04-18T15:09:44.069804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos_transposed = np.transpose(videos, (1, 0, 2))\nX = np.split(videos_transposed, 4000 // 100, axis=0)\nX = np.array(X)\nX = np.transpose(X, (2, 0, 1, 3))\nX = np.array(X)\nprint(X.shape)","metadata":{"id":"t5oCZAXaWDyZ","outputId":"a4ffe8ac-7942-4bb6-80e5-e9fd86dbbd7a","execution":{"iopub.status.busy":"2024-04-18T15:09:44.071518Z","iopub.execute_input":"2024-04-18T15:09:44.071781Z","iopub.status.idle":"2024-04-18T15:09:44.255831Z","shell.execute_reply.started":"2024-04-18T15:09:44.071749Z","shell.execute_reply":"2024-04-18T15:09:44.254841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"pA-WaIDrczT6","outputId":"ecbf03cd-a361-4224-d178-bc51cae42e9d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # conv block\n        self.conv2Dblock = nn.Sequential(\n            # 1. conv block\n            nn.Conv2d(in_channels=1,\n                                   out_channels=16,\n                                   kernel_size=3,\n                                   stride=1,\n                                   padding=1\n                                  ),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.2),\n            # 2. conv block\n            nn.Conv2d(in_channels=16,\n                                   out_channels=32,\n                                   kernel_size=3,\n                                   stride=1,\n                                   padding=1\n                                  ),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.2),\n            # 3. conv block\n            nn.Conv2d(in_channels=32,\n                                   out_channels=64,\n                                   kernel_size=3,\n                                   stride=1,\n                                   padding=1\n                                  ),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.2)\n        )\n        # LSTM block\n        hidden_size = 128\n        self.lstm = nn.LSTM(input_size=192,hidden_size=hidden_size,bidirectional=True, batch_first=True)\n        self.dropout_lstm = nn.Dropout(p=0.3)\n        self.attention_linear = nn.Linear(2*hidden_size,1) # 2*hidden_size for the 2 outputs of bidir LSTM\n        # Linear softmax layer\n        self.out_linear = nn.Linear(2*hidden_size,1)\n    def forward(self,x):\n        batch_size, time_steps, height, width = x.size()\n        x = x.view(batch_size * time_steps, 1, height, width)\n        conv_embedding = self.conv2Dblock(x)\n        conv_embedding = conv_embedding.view(batch_size, time_steps, -1, conv_embedding.size(2), conv_embedding.size(3))\n        conv_embedding = torch.flatten(conv_embedding, start_dim=2) # do not flatten batch dimension and time\n        lstm_embedding, (h,c) = self.lstm(conv_embedding)\n        lstm_embedding = self.dropout_lstm(lstm_embedding)\n        # lstm_embedding (batch, time, hidden_size*2)\n        batch_size,T,_ = lstm_embedding.shape\n        attention_weights = [None]*T\n        for t in range(T):\n            embedding = lstm_embedding[:,t,:]\n            attention_weights[t] = self.attention_linear(embedding)\n        attention_weights_norm = nn.functional.softmax(torch.stack(attention_weights,-1),dim=-1)\n        attention = torch.bmm(attention_weights_norm,lstm_embedding) # (Bx1xT)*(B,T,hidden_size*2)=(B,1,2*hidden_size)\n        attention = torch.squeeze(attention, 1)\n        prediction = self.out_linear(attention)\n        \n        return prediction","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:44.257029Z","iopub.execute_input":"2024-04-18T15:09:44.257320Z","iopub.status.idle":"2024-04-18T15:09:44.274410Z","shell.execute_reply.started":"2024-04-18T15:09:44.257295Z","shell.execute_reply":"2024-04-18T15:09:44.273541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def loss_fnc(predictions, targets):\n#     return nn.MSELoss()(input=predictions,target=targets)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:44.275805Z","iopub.execute_input":"2024-04-18T15:09:44.276161Z","iopub.status.idle":"2024-04-18T15:09:44.288287Z","shell.execute_reply.started":"2024-04-18T15:09:44.276129Z","shell.execute_reply":"2024-04-18T15:09:44.287392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def make_train_step(model, loss_fnc, optimizer):\n#     def train_step(X,Y):\n#         # set model to train mode\n#         model.train()\n#         # forward pass\n#         output = model(X)\n#         # compute loss\n#         loss = loss_fnc(output, Y)\n#         # compute gradients\n#         loss.backward()\n#         # update parameters and zero gradients\n#         optimizer.step()\n#         optimizer.zero_grad()\n#         return loss.item()\n\n#     return train_step","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:44.289359Z","iopub.execute_input":"2024-04-18T15:09:44.289622Z","iopub.status.idle":"2024-04-18T15:09:44.298383Z","shell.execute_reply.started":"2024-04-18T15:09:44.289598Z","shell.execute_reply":"2024-04-18T15:09:44.297516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:44.299363Z","iopub.execute_input":"2024-04-18T15:09:44.299684Z","iopub.status.idle":"2024-04-18T15:09:44.406105Z","shell.execute_reply.started":"2024-04-18T15:09:44.299658Z","shell.execute_reply":"2024-04-18T15:09:44.405299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:44.407579Z","iopub.execute_input":"2024-04-18T15:09:44.407890Z","iopub.status.idle":"2024-04-18T15:09:44.413913Z","shell.execute_reply.started":"2024-04-18T15:09:44.407863Z","shell.execute_reply":"2024-04-18T15:09:44.412951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nEPOCHS=500\nDATASET_SIZE = X_train.shape[0]\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Selected device is {}'.format(device))\nmodel = MyModel().to(device)\nprint('Number of trainable params: ',sum(p.numel() for p in model.parameters()))\n\noptimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n# train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\nloss_fnc = nn.MSELoss()\nlosses=[]\nX_tensor = torch.tensor(X_train, device=device).float()\nY_tensor = torch.tensor(Y_train, device=device).float()\nmodel.train()\nfor epoch in range(EPOCHS):\n    # forward pass\n    output = model(X_tensor)\n    # compute loss\n    loss = loss_fnc(output, Y_tensor)\n    # compute gradients\n    loss.backward()\n    # update parameters and zero gradients\n    optimizer.step()\n    optimizer.zero_grad()\n    losses.append(loss.item())\n#     print('')\n    print(f\"Epoch {epoch} --> loss:{loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:09:44.415014Z","iopub.execute_input":"2024-04-18T15:09:44.415354Z","iopub.status.idle":"2024-04-18T15:13:38.275931Z","shell.execute_reply.started":"2024-04-18T15:09:44.415327Z","shell.execute_reply":"2024-04-18T15:13:38.273308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import OneHotEncoder\n# scaler = MinMaxScaler()\n# X = scaler.fit_transform(X)\n# X = X.reshape(len(videos), max_frames, all_videos.shape[1]-1)\n\n# onehot_encoder = OneHotEncoder(sparse=False)\n# y = onehot_encoder.fit_transform(y.reshape(-1, 1))\n# y=to_categorical(y)\n# num_classes = len(np.unique(y))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:13:38.277110Z","iopub.execute_input":"2024-04-18T15:13:38.277455Z","iopub.status.idle":"2024-04-18T15:13:38.283309Z","shell.execute_reply.started":"2024-04-18T15:13:38.277411Z","shell.execute_reply":"2024-04-18T15:13:38.282152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(X))\n# print(len(X[0]))\n# print(X[0][0].shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:13:38.285128Z","iopub.execute_input":"2024-04-18T15:13:38.285811Z","iopub.status.idle":"2024-04-18T15:13:38.303034Z","shell.execute_reply.started":"2024-04-18T15:13:38.285775Z","shell.execute_reply":"2024-04-18T15:13:38.301999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define the data iterator function\n# def data_iterator(feature_matrix, labels, batch_size=32):\n#     num_samples = feature_matrix.shape[0]\n#     num_batches = num_samples // batch_size\n\n#     while True:\n#         # Shuffle the data at the beginning of each epoch\n#         indices = np.random.permutation(num_samples)\n#         feature_matrix = feature_matrix[indices]\n#         labels = labels[indices]\n\n#         for batch_idx in range(num_batches):\n#             start_idx = batch_idx * batch_size\n#             end_idx = (batch_idx + 1) * batch_size\n#             batch_x = feature_matrix[start_idx:end_idx]\n#             batch_y = labels[start_idx:end_idx]\n#             yield batch_x, batch_y\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:13:38.304158Z","iopub.execute_input":"2024-04-18T15:13:38.305138Z","iopub.status.idle":"2024-04-18T15:13:38.315788Z","shell.execute_reply.started":"2024-04-18T15:13:38.305096Z","shell.execute_reply":"2024-04-18T15:13:38.314924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# #*******************\n# # Define inputs for attention and LSTM\n# X=np.stack(X)\n# input_data = Input(shape=(max_frames, image_size, image_size, 1))  # Input shape (time steps, features)\n# conv_output = Conv3D(filters=2, kernel_size=(7, 7, 7), activation='relu')(input_data)\n# conv_output = Conv3D(filters=2, kernel_size=(7, 7, 7), activation='relu')(conv_output)\n# conv_output = Conv3D(filters=2, kernel_size=(5, 5, 5), activation='relu')(conv_output)\n# conv_output = Conv3D(filters=2, kernel_size=(5, 5, 5), activation='relu')(conv_output)\n# conv_output = Conv3D(filters=2, kernel_size=(3, 3, 3), activation='relu')(conv_output)\n# conv_output = Conv3D(filters=2, kernel_size=(3, 3, 3), activation='relu')(conv_output)\n# reshaped_output = Reshape((-1, 2 * 6 * 6))(conv_output)\n\n# # Attention block\n# attn_output = MultiHeadAttention(num_heads=2, key_dim=2 * 6 * 6)(reshaped_output, reshaped_output)\n# attn_output = attn_output + reshaped_output  # Add skip connection\n\n# # LSTM layer\n# lstm_output = LSTM(32)(attn_output)  # Define the number of LSTM units and adjust as needed\n\n# # Define the number of output classes\n# # num_classes = y.shape[1]  # Number of columns in the one-hot encoded label matrix\n\n# # Dense layers for classification\n# output = Dense(1)(lstm_output)\n\n# # Define the model\n# model = Model(inputs=input_data, outputs=output)\n# #*******************\n\n# model.compile(loss='mean_squared_error', optimizer='adam')\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# # Train the model\n# model.fit(X_train, y_train, epochs=10, batch_size=32)\n\n# # batch_size = 32\n# # epochs = 10\n# # data_gen = data_iterator(X_train, y_train, batch_size)\n# # steps_per_epoch = X_train.shape[0] // batch_size\n\n\n\n# # Evaluate the model\n# mse = model.evaluate(X_test, y_test)\n# print(\"Mean Squared Error:\", mse)\n\n\n# for epoch in range(epochs):\n#     print('Epoch:', epoch + 1)\n    \n#     # Train the model for one epoch\n#     model.fit(data_gen, steps_per_epoch=steps_per_epoch)\n\n# predictions = model.predict(X_test)\n# predicted_labels = np.argmax(predictions, axis=1)\n# # Calculate test accuracy\n# accuracy = accuracy_score(np.argmax(y_test, axis=1), predicted_labels)\n# print(\"Test Accuracy:\", accuracy)","metadata":{"id":"Ch6a4C51wpv8","outputId":"2f880039-73b9-4cd1-e048-d4767e4ca0f2","execution":{"iopub.status.busy":"2024-04-18T15:13:38.317078Z","iopub.execute_input":"2024-04-18T15:13:38.317399Z","iopub.status.idle":"2024-04-18T15:13:38.328075Z","shell.execute_reply.started":"2024-04-18T15:13:38.317369Z","shell.execute_reply":"2024-04-18T15:13:38.327101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}